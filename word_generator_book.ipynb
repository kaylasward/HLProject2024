{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from read_tab_files import TabFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a ti\n",
      "a'san\n",
      "a'san sili\n",
      "a'tan\n",
      "aawa\n",
      "adyu\n",
      "afu\n",
      "aj\n",
      "ajkesha\n",
      "ajpa\n",
      "akin\n",
      "aman\n",
      "amana\n",
      "amta\n",
      "an\n",
      "ana\n",
      "ani\n",
      "ani srÉ¨\n",
      "anku\n"
     ]
    }
   ],
   "source": [
    "barb_forms = TabFileReader.tab_reader(\n",
    "    \"chl2024_barbacoandata/chl2023_barbacoan_forms.tab\"\n",
    ")\n",
    "\n",
    "barb_word_list = TabFileReader.get_word_list(barb_forms)\n",
    "sorted_barb_word_list = sorted(list(set(barb_word_list)))\n",
    "for word in sorted_barb_word_list[0:20]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to character sequences\n",
    "tokens = [list(word) for word in barb_word_list]\n",
    "\n",
    "# flatten the list of chars to fit LabelEncoder\n",
    "flat_tokens = [item for sublist in tokens for item in sublist]\n",
    "\n",
    "# encode characters as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(flat_tokens)\n",
    "encoded_tokens = [encoder.transform(word) for word in tokens]\n",
    "\n",
    "# specify sequence length (how long each seq should be interpreted as)\n",
    "sequence_length = 3\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# go through each word and get every sequence with the seq len (hello > hel, ell, llo if seq_len = 3)\n",
    "for seq in encoded_tokens:\n",
    "    for i in range(sequence_length, len(seq)):\n",
    "        X.append(seq[i - sequence_length : i])\n",
    "        y.append(seq[i])\n",
    "\n",
    "\n",
    "# prep data for training\n",
    "X = np.array(X)\n",
    "# reshape X to fit the LSTM input [samples, time steps, features]\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 2ms/step - loss: 3.0386\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7972\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7514\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7311\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7131\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6907\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6766\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6671\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6497\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6317\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6259\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6016\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6012\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5839\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5779\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.5826\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5743\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.5572\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.5521\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5405\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.5310\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.5305\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.5242\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.5250\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5114\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5107\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5042\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4979\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4901\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4842\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4776\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.4719\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4747\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4732\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4634\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4680\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4477\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4404\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4353\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4320\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4273\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4257\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4209\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4084\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4086\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3987\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4064\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3913\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3888\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3771\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3692\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3698\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3686\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3639\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.3607\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3411\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3385\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3419\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3360\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.3316\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3349\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3254\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3164\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3211\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3035\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.3058\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2964\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3044\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2885\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3008\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.2973\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2885\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2716\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2735\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2714\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.2527\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.2530\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2534\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.2649\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2497\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2412\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2362\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2433\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2431\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2294\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2311\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.2268\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2257\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2143\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2084\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2140\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2203\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.2017\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2058\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.2111\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1895\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1818\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1924\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1885\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1580d1610>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barb_model = Sequential(\n",
    "    [\n",
    "        # memory units (50 is good starting num), seq_len, step size (1)\n",
    "        LSTM(100, input_shape=(X.shape[1], X.shape[2])),\n",
    "        Dropout(0.2),\n",
    "        # num unique chars\n",
    "        Dense(y.shape[1], activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "barb_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "barb_model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"english_word_list.txt\", \"r\") as file:\n",
    "    english_word_list = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to character sequences\n",
    "eng_tokens = [list(word) for word in english_word_list]\n",
    "\n",
    "# flatten the list of chars to fit LabelEncoder\n",
    "eng_flat_tokens = [item for sublist in eng_tokens for item in sublist]\n",
    "\n",
    "# encode characters as integers\n",
    "eng_encoder = LabelEncoder()\n",
    "eng_encoder.fit(eng_flat_tokens)\n",
    "eng_encoded_tokens = [eng_encoder.transform(word) for word in eng_tokens]\n",
    "\n",
    "# specify sequence length (how long each seq should be interpreted as)\n",
    "eng_sequence_length = 3\n",
    "\n",
    "eng_X = []\n",
    "eng_y = []\n",
    "\n",
    "# go through each word and get every sequence with the seq len (hello > hel, ell, llo if seq_len = 3)\n",
    "for seq in eng_encoded_tokens:\n",
    "    for i in range(eng_sequence_length, len(seq)):\n",
    "        eng_X.append(seq[i - eng_sequence_length : i])\n",
    "        eng_y.append(seq[i])\n",
    "\n",
    "\n",
    "# prep data for training\n",
    "eng_X = np.array(eng_X)\n",
    "# reshape X to fit the LSTM input [samples, time steps, features]\n",
    "eng_X = np.reshape(eng_X, (eng_X.shape[0], eng_X.shape[1], 1))\n",
    "eng_y = to_categorical(eng_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "59/59 [==============================] - 1s 2ms/step - loss: 2.9310\n",
      "Epoch 2/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7879\n",
      "Epoch 3/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7639\n",
      "Epoch 4/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7584\n",
      "Epoch 5/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7475\n",
      "Epoch 6/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7394\n",
      "Epoch 7/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7372\n",
      "Epoch 8/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7294\n",
      "Epoch 9/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7244\n",
      "Epoch 10/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7235\n",
      "Epoch 11/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7228\n",
      "Epoch 12/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7114\n",
      "Epoch 13/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7108\n",
      "Epoch 14/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7085\n",
      "Epoch 15/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7079\n",
      "Epoch 16/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7047\n",
      "Epoch 17/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.7094\n",
      "Epoch 18/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6978\n",
      "Epoch 19/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6994\n",
      "Epoch 20/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6951\n",
      "Epoch 21/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6923\n",
      "Epoch 22/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6909\n",
      "Epoch 23/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6850\n",
      "Epoch 24/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6779\n",
      "Epoch 25/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6800\n",
      "Epoch 26/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6794\n",
      "Epoch 27/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6777\n",
      "Epoch 28/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6732\n",
      "Epoch 29/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6660\n",
      "Epoch 30/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6703\n",
      "Epoch 31/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6614\n",
      "Epoch 32/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6577\n",
      "Epoch 33/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6563\n",
      "Epoch 34/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6483\n",
      "Epoch 35/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6624\n",
      "Epoch 36/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6482\n",
      "Epoch 37/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6409\n",
      "Epoch 38/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6341\n",
      "Epoch 39/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6369\n",
      "Epoch 40/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6357\n",
      "Epoch 41/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6274\n",
      "Epoch 42/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6226\n",
      "Epoch 43/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6304\n",
      "Epoch 44/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6218\n",
      "Epoch 45/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6163\n",
      "Epoch 46/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6107\n",
      "Epoch 47/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6115\n",
      "Epoch 48/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6125\n",
      "Epoch 49/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5943\n",
      "Epoch 50/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.6029\n",
      "Epoch 51/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5905\n",
      "Epoch 52/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5836\n",
      "Epoch 53/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5877\n",
      "Epoch 54/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5877\n",
      "Epoch 55/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5789\n",
      "Epoch 56/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5777\n",
      "Epoch 57/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5694\n",
      "Epoch 58/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5609\n",
      "Epoch 59/100\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 2.5637\n",
      "Epoch 60/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5669\n",
      "Epoch 61/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5579\n",
      "Epoch 62/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5590\n",
      "Epoch 63/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5451\n",
      "Epoch 64/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5512\n",
      "Epoch 65/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5483\n",
      "Epoch 66/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5426\n",
      "Epoch 67/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5394\n",
      "Epoch 68/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5412\n",
      "Epoch 69/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5336\n",
      "Epoch 70/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5310\n",
      "Epoch 71/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5288\n",
      "Epoch 72/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5184\n",
      "Epoch 73/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5128\n",
      "Epoch 74/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5194\n",
      "Epoch 75/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5220\n",
      "Epoch 76/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5085\n",
      "Epoch 77/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5115\n",
      "Epoch 78/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5075\n",
      "Epoch 79/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5016\n",
      "Epoch 80/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4957\n",
      "Epoch 81/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4918\n",
      "Epoch 82/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.5028\n",
      "Epoch 83/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4992\n",
      "Epoch 84/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4961\n",
      "Epoch 85/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4965\n",
      "Epoch 86/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4845\n",
      "Epoch 87/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4734\n",
      "Epoch 88/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4839\n",
      "Epoch 89/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4694\n",
      "Epoch 90/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4768\n",
      "Epoch 91/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4739\n",
      "Epoch 92/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4723\n",
      "Epoch 93/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4735\n",
      "Epoch 94/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4649\n",
      "Epoch 95/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4653\n",
      "Epoch 96/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4581\n",
      "Epoch 97/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4577\n",
      "Epoch 98/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4617\n",
      "Epoch 99/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4586\n",
      "Epoch 100/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 2.4540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x15cb85a10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_model = Sequential(\n",
    "    [\n",
    "        # memory units (50 is good starting num), seq_len, step size (1)\n",
    "        LSTM(100, input_shape=(eng_X.shape[1], eng_X.shape[2])),\n",
    "        Dropout(0.2),\n",
    "        # num unique chars\n",
    "        Dense(eng_y.shape[1], activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "english_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "english_model.fit(eng_X, eng_y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word(model, input_text, extend_length, encoder):\n",
    "    # pad text if its length is less than seq_len\n",
    "    if len(input_text) < sequence_length:\n",
    "        input_text = \" \" * (sequence_length - len(input_text)) + input_text\n",
    "\n",
    "    for _ in range(extend_length):\n",
    "        encoded = encoder.transform(list(input_text[-sequence_length:]))\n",
    "        encoded = np.reshape(encoded, (1, sequence_length, 1))\n",
    "\n",
    "        # predict next character\n",
    "        pred = model.predict(encoded, verbose=0)\n",
    "        next_char = encoder.inverse_transform([np.argmax(pred)])\n",
    "        input_text += next_char[0]\n",
    "\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cognates = [\"pi\", \"ku\", \"mÉ¨\", \"lul\"]\n",
    "lengths = list(range(1, 6))\n",
    "\n",
    "for cognate in cognates:\n",
    "    print(\"Cognate:\", cognate)\n",
    "    for length in lengths:\n",
    "        new_word = generate_word(barb_model, cognate, length, encoder)\n",
    "        print(\"  Extra Length:\", length)\n",
    "        print(\"  Generated Word:\", new_word)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated word:  amra\n"
     ]
    }
   ],
   "source": [
    "new_word = generate_word(barb_model, \"am\", 2, encoder)\n",
    "print(\"Generated word:\", new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated word: there\n"
     ]
    }
   ],
   "source": [
    "new_word = generate_word(english_model, \"the\", 2, eng_encoder)\n",
    "print(\"Generated word:\", new_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
